{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nafilahhans/ML/blob/main/repo3_03_asl_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfheRhwtM4cD"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glkA2PEKM4cO"
      },
      "source": [
        "## Loading and Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW6qG8pFM4cR"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as keras\n",
        "import pandas as pd\n",
        "\n",
        "# Load in our data from CSV files\n",
        "train_df = pd.read_csv(\"data/asl_data/sign_mnist_train.csv\")\n",
        "valid_df = pd.read_csv(\"data/asl_data/sign_mnist_valid.csv\")\n",
        "\n",
        "# Separate out our target values\n",
        "y_train = train_df['label']\n",
        "y_valid = valid_df['label']\n",
        "del train_df['label']\n",
        "del valid_df['label']\n",
        "\n",
        "# Separate out our image vectors\n",
        "x_train = train_df.values\n",
        "x_valid = valid_df.values\n",
        "\n",
        "# Turn our scalar targets into binary categories\n",
        "num_classes = 24\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_valid = keras.utils.to_categorical(y_valid, num_classes)\n",
        "\n",
        "# Normalize our image data\n",
        "x_train = x_train / 255\n",
        "x_valid = x_valid / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttDO5HumM4cU"
      },
      "source": [
        "## Reshaping Images for a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ1P45NwM4cV"
      },
      "source": [
        "In the last exercise, the individual pictures in our dataset are in the format of long lists of 784 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wGxUrm8M4cW",
        "outputId": "4ddad0ba-044c-4558-99c2-d774d8494df6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((27455, 784), (7172, 784))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape, x_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MhHYi4NM4cY"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape(-1,28,28,1)\n",
        "x_valid = x_valid.reshape(-1,28,28,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVCvPARaM4cZ",
        "outputId": "3c0d5102-73ca-4efa-d852-416c6ffadefe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(27455, 28, 28, 1)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLVjs_WnM4ca",
        "outputId": "99686bc1-f243-4a4d-ce1e-2829524ca1df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7172, 28, 28, 1)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpX26QIOM4ca",
        "outputId": "56b9b544-d138-45ca-a438-a8f614f48c63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((27455, 28, 28, 1), (7172, 28, 28, 1))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train.shape, x_valid.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdWHGsr0M4cb"
      },
      "source": [
        "## Creating a Convolutional Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj_ETShIM4cb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        "    Conv2D,\n",
        "    MaxPool2D,\n",
        "    Flatten,\n",
        "    Dropout,\n",
        "    BatchNormalization,\n",
        ")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\",\n",
        "                 input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
        "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
        "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=512, activation=\"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(units=num_classes, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC49vNAcM4cc"
      },
      "source": [
        "### [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOvKc1rqM4ce"
      },
      "source": [
        "## Summarizing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcXXy5grM4cf",
        "outputId": "ba74e1bd-6831-427c-eb36-088a34fc7204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 75)        750       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 28, 28, 75)        300       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 75)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 50)        33800     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 50)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 14, 14, 50)        200       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 50)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 25)          11275     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 25)          100       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 25)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               205312    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 24)                12312     \n",
            "=================================================================\n",
            "Total params: 264,049\n",
            "Trainable params: 263,749\n",
            "Non-trainable params: 300\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "konUKkyJM4cf"
      },
      "source": [
        "## Compiling the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z2P3_EeM4cg"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFVFQT3TM4ci"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGKWvgWJM4ct",
        "outputId": "f0ba2485-6d7a-4cbc-e470-980699d2fa10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "858/858 [==============================] - 5s 6ms/step - loss: 0.2938 - accuracy: 0.9109 - val_loss: 0.5968 - val_accuracy: 0.8353\n",
            "Epoch 2/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0199 - accuracy: 0.9934 - val_loss: 0.3019 - val_accuracy: 0.9208\n",
            "Epoch 3/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0123 - accuracy: 0.9963 - val_loss: 0.4271 - val_accuracy: 0.9069\n",
            "Epoch 4/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.6193 - val_accuracy: 0.8783\n",
            "Epoch 5/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.4555 - val_accuracy: 0.8950\n",
            "Epoch 6/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.2768 - val_accuracy: 0.9492\n",
            "Epoch 7/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.3857 - val_accuracy: 0.9244\n",
            "Epoch 8/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.2602 - val_accuracy: 0.9523\n",
            "Epoch 9/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.5741 - val_accuracy: 0.9028\n",
            "Epoch 10/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.3757 - val_accuracy: 0.9516\n",
            "Epoch 11/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.3797 - val_accuracy: 0.9467\n",
            "Epoch 12/20\n",
            "858/858 [==============================] - 4s 5ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.3811 - val_accuracy: 0.9419\n",
            "Epoch 13/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.3025 - val_accuracy: 0.9578\n",
            "Epoch 14/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 6.6945e-04 - accuracy: 0.9998 - val_loss: 0.3700 - val_accuracy: 0.9434\n",
            "Epoch 15/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 9.4940e-04 - accuracy: 0.9996 - val_loss: 0.2736 - val_accuracy: 0.9647\n",
            "Epoch 16/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3283 - val_accuracy: 0.9640\n",
            "Epoch 17/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.4090 - val_accuracy: 0.9565\n",
            "Epoch 18/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.6200 - val_accuracy: 0.9382\n",
            "Epoch 19/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.3763 - val_accuracy: 0.9513\n",
            "Epoch 20/20\n",
            "858/858 [==============================] - 4s 4ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.4008 - val_accuracy: 0.9621\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f73409ed978>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train, y_train, epochs=20, verbose=1, validation_data=(x_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTzqnNDiM4cy",
        "outputId": "bffe8ed4-1b9f-4e20-a3c0-f3c65991a5d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}